{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8bcb2e3957e345b29be2c3ef71ed266a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ff9b18cbebd4e3791fc3c5b3215e3e6",
              "IPY_MODEL_0ca97c4df6e4463888731350b9088815",
              "IPY_MODEL_57d857614eac43959acb987cf5bfea9e"
            ],
            "layout": "IPY_MODEL_345618ebdc3d466db912a9bfd8a67961"
          }
        },
        "8ff9b18cbebd4e3791fc3c5b3215e3e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c74107b0d0c94b1d83676b76c0470460",
            "placeholder": "​",
            "style": "IPY_MODEL_6add67704a114d9d8e2fb44c577842df",
            "value": "Map: 100%"
          }
        },
        "0ca97c4df6e4463888731350b9088815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4455bbdbedd14f6da017be42174cc13d",
            "max": 3500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb9ca7ea6da64c13b8bbcb3ab5987648",
            "value": 3500
          }
        },
        "57d857614eac43959acb987cf5bfea9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6969dbd18a204cf1acaf274c6b3c22ee",
            "placeholder": "​",
            "style": "IPY_MODEL_bcf37077dbd7450bbcd6513de408fad0",
            "value": " 3500/3500 [00:25&lt;00:00, 135.34 examples/s]"
          }
        },
        "345618ebdc3d466db912a9bfd8a67961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c74107b0d0c94b1d83676b76c0470460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6add67704a114d9d8e2fb44c577842df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4455bbdbedd14f6da017be42174cc13d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb9ca7ea6da64c13b8bbcb3ab5987648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6969dbd18a204cf1acaf274c6b3c22ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcf37077dbd7450bbcd6513de408fad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c29da8fd332044fda0a13afdb89a19b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_551d0c46e2a54beb93df29821a245ad4",
              "IPY_MODEL_459700b03f2f46b0ba72a437ba7cc981",
              "IPY_MODEL_2c218d17baaa4adaa7a0e0c988850164"
            ],
            "layout": "IPY_MODEL_654caf5dd9924af79bf8e79244a8ac44"
          }
        },
        "551d0c46e2a54beb93df29821a245ad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec2a5d295fa487f82c0692d2eac6e95",
            "placeholder": "​",
            "style": "IPY_MODEL_2fb4b8e8a9294b7ead634fc7a50df757",
            "value": "Map: 100%"
          }
        },
        "459700b03f2f46b0ba72a437ba7cc981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbb42b40cb0f4a7ca4e2c4fccc53cbfc",
            "max": 1500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e32712802ef7407fb1bb4379c792e808",
            "value": 1500
          }
        },
        "2c218d17baaa4adaa7a0e0c988850164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b73db07a690439990f86d6f308b8a4d",
            "placeholder": "​",
            "style": "IPY_MODEL_f5ba0bdd5cfa4155a826c5f27b7ab51b",
            "value": " 1500/1500 [00:10&lt;00:00, 139.64 examples/s]"
          }
        },
        "654caf5dd9924af79bf8e79244a8ac44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fec2a5d295fa487f82c0692d2eac6e95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fb4b8e8a9294b7ead634fc7a50df757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cbb42b40cb0f4a7ca4e2c4fccc53cbfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e32712802ef7407fb1bb4379c792e808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b73db07a690439990f86d6f308b8a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5ba0bdd5cfa4155a826c5f27b7ab51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923,
          "referenced_widgets": [
            "8bcb2e3957e345b29be2c3ef71ed266a",
            "8ff9b18cbebd4e3791fc3c5b3215e3e6",
            "0ca97c4df6e4463888731350b9088815",
            "57d857614eac43959acb987cf5bfea9e",
            "345618ebdc3d466db912a9bfd8a67961",
            "c74107b0d0c94b1d83676b76c0470460",
            "6add67704a114d9d8e2fb44c577842df",
            "4455bbdbedd14f6da017be42174cc13d",
            "cb9ca7ea6da64c13b8bbcb3ab5987648",
            "6969dbd18a204cf1acaf274c6b3c22ee",
            "bcf37077dbd7450bbcd6513de408fad0",
            "c29da8fd332044fda0a13afdb89a19b7",
            "551d0c46e2a54beb93df29821a245ad4",
            "459700b03f2f46b0ba72a437ba7cc981",
            "2c218d17baaa4adaa7a0e0c988850164",
            "654caf5dd9924af79bf8e79244a8ac44",
            "fec2a5d295fa487f82c0692d2eac6e95",
            "2fb4b8e8a9294b7ead634fc7a50df757",
            "cbb42b40cb0f4a7ca4e2c4fccc53cbfc",
            "e32712802ef7407fb1bb4379c792e808",
            "0b73db07a690439990f86d6f308b8a4d",
            "f5ba0bdd5cfa4155a826c5f27b7ab51b"
          ]
        },
        "id": "O820y41MJCMA",
        "outputId": "de762c2d-cb9e-4984-ab43-8b0ff790dc69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bcb2e3957e345b29be2c3ef71ed266a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c29da8fd332044fda0a13afdb89a19b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-1691528197>:103: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler()\n",
            "<ipython-input-6-1691528197>:115: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Training Loss: 1.4651\n",
            "Epoch 2 | Training Loss: 0.8090\n",
            "Epoch 3 | Training Loss: 0.4207\n",
            "ROUGE Scores: {'rouge1': np.float64(0.4272428475802083), 'rouge2': np.float64(0.21497819244885583), 'rougeL': np.float64(0.31126794764789745), 'rougeLsum': np.float64(0.40353885153290947)}\n",
            "Generated Summary:\n",
            " Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 on Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
            "Actor says he is keeping his feet firmly on the ground despite his growing fame .\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, get_scheduler\n",
        "from datasets import load_dataset\n",
        "!pip install evaluate\n",
        "from evaluate import load\n",
        "\n",
        "# Load Dataset (First 1000 samples)\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the full train split as a stream\n",
        "dataset_stream = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\", streaming=True)\n",
        "\n",
        "# Manually collect the first 1000 samples into a list\n",
        "dataset = [x for _, x in zip(range(5000), dataset_stream)]\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_list(dataset)\n",
        "\n",
        "train_valid = dataset.train_test_split(test_size=0.3)\n",
        "\n",
        "# Tokenizer and Model (Using BART Large CNN)\n",
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "# Pointer-Generator Mechanism\n",
        "class PointerGenerator(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.pointer = nn.Linear(hidden_dim * 2, 1)\n",
        "\n",
        "    def forward(self, decoder_hidden, context_vector):\n",
        "        concat = torch.cat((decoder_hidden, context_vector), dim=-1)\n",
        "        p_gen = torch.sigmoid(self.pointer(concat))\n",
        "        return p_gen\n",
        "\n",
        "# Summarization Model with PGN\n",
        "class BartWithPointerGenerator(nn.Module):\n",
        "    def __init__(self, bart_model):\n",
        "        super().__init__()\n",
        "        self.bart = bart_model\n",
        "        self.pointer_generator = PointerGenerator(hidden_dim=1024)  # BART Large hidden size\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, decoder_input_ids, labels=None):\n",
        "        encoder_outputs = self.bart.model.encoder(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        decoder_outputs = self.bart.model.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            encoder_hidden_states=encoder_outputs.last_hidden_state,\n",
        "            encoder_attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        logits = self.bart.lm_head(decoder_outputs.last_hidden_state)\n",
        "\n",
        "        p_gen = self.pointer_generator(\n",
        "            decoder_outputs.last_hidden_state[:, -1, :],\n",
        "            encoder_outputs.last_hidden_state[:, 0, :]\n",
        "        )\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
        "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "        return {\"loss\": loss, \"logits\": logits, \"p_gen\": p_gen}\n",
        "\n",
        "# Initialize Model\n",
        "model = BartWithPointerGenerator(bart_model)\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess(example):\n",
        "    inputs = tokenizer(example[\"article\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
        "    outputs = tokenizer(example[\"highlights\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "    inputs[\"labels\"] = outputs[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "train_dataset = train_valid[\"train\"].map(preprocess, batched=True)\n",
        "valid_dataset = train_valid[\"test\"].map(preprocess, batched=True)\n",
        "\n",
        "# DataLoaders\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'input_ids': torch.tensor([f[\"input_ids\"] for f in batch]),\n",
        "        'attention_mask': torch.tensor([f[\"attention_mask\"] for f in batch]),\n",
        "        'decoder_input_ids': torch.tensor([f[\"labels\"][:-1] for f in batch]),\n",
        "        'labels': torch.tensor([f[\"labels\"][1:] for f in batch])\n",
        "    }\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)  # Reduce batch size if needed\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=len(train_loader) * 3  # 3 epochs\n",
        ")\n",
        "\n",
        "# Mixed Precision Training\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Training Loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs[\"loss\"]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1} | Training Loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "# Save Model\n",
        "torch.save(model.state_dict(), \"bart_large_pointer_generator_cnn1000.pth\")\n",
        "\n",
        "# ROUGE Evaluation\n",
        "metric = load(\"rouge\")\n",
        "model.eval()\n",
        "predictions, references = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in valid_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        generated_ids = model.bart.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "        decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        decoded_labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "        predictions.extend(decoded_preds)\n",
        "        references.extend(decoded_labels)\n",
        "\n",
        "rouge_scores = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n",
        "\n",
        "# Inference Example\n",
        "def summarize(text, model, tokenizer, max_length=1024):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.bart.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            min_length=50,\n",
        "            num_beams=6,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Test Inference\n",
        "sample_text = dataset[0][\"article\"]\n",
        "print(\"Generated Summary:\\n\", summarize(sample_text, model, tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference Example\n",
        "def summarize(text, model, tokenizer, max_length=1024):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.bart.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=max_length,\n",
        "            min_length=50,\n",
        "            num_beams=6,\n",
        "            length_penalty=2.0,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Test Inference\n",
        "sample_text = dataset[0][\"article\"]\n",
        "print(\"Generated Summary:\\n\", summarize(sample_text, model, tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOOqzozlNsJm",
        "outputId": "032555eb-9166-408e-ebf5-8655562f7751"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            " Harry Potter star Daniel Radcliffe gets £20M fortune as he turns 18 on Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n",
            "The Londoner has filmed a TV movie called \"My Boy Jack,\" due for release later this year .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "Abstractive Text Summarization: A Transformer Based Approach\n",
        "\n",
        "Anushka R Kale Department of Computer Engineering COEP Technological University Pune, India kanushka2104@gmail.com\n",
        "\n",
        "Pratiksha R Deshmukh Department of Computer Engineering COEP Technological University Pune, India dpr.comp@coep.ac.in\n",
        "\n",
        "Abstract -This research delves into the difficulty of summarizing legal documents using Natural Language Processing. It examines how cutting-edge models like XLNet and BART can be used for abstractive summarization specifically tailored for lengthy legal cases. The study assesses these models' abilities to condense complex legal texts, highlighting the constraints imposed by input token limits. Through a thorough comparison of XLNet and BART based on legal-specific standards, the research introduces a fresh approach to improve summarization by leveraging these models' strengths while addressing their limitations. Evaluation methods include ROUGE scores. This study advances our understanding of abstractive summarization, particularly in the realm of legal texts, offering valuable insights for both legal professionals and NLP researchers.\n",
        "\n",
        "Keywords -Summarization, Legal, Abstractive, XLNet, BART, ROUGE Score\n",
        "\n",
        "I. INTRODUCTION\n",
        "\n",
        "Abstractive text summarization is a crucial application of Natural Language Processing (NLP) in condensing lengthy texts into coherent and informative summaries. However, the challenge of efficient legal text summarization lies in the unique characteristics of legal documents, such as their length and specialized terminology. There are currently two types of summarizations: extractive summarization, which extracts significant phrases or sentences from a lengthy text, and abstractive summarization, which paraphrases a lengthy sentence while maintaining its meaning.\n",
        "\n",
        "This research uses advanced AI and Machine Learning models, specifically XLNet and BART, to tackle the intricacies of abstractive summarization within the legal domain. The traditional, labor-intensive process of manual case summarization can be revolutionized with the aid of state-of-the-art machine learning models, saving substantial time and resources. With over 4.70 crore pending legal cases in India alone, automatic summarization holds the potential to streamline and expedite legal proceedings significantly.\n",
        "\n",
        "The study proposes a novel methodology for Indian legal texts, focusing on extractive and abstractive text summarization. By normalizing Indian legal texts and utilizing domainindependent models, the work offers a fresh perspective on the effectiveness of XLNet and BART in transforming lengthy legal documents into succinct, informative summaries.\n",
        "\n",
        "The major contribution of this work is the use of simple processing that is well suited to the XLnet and BART models. The compressed documents are then fed to BART to generate concise and coherent summaries, demonstrating how this method works well for enhancing the summarization of legal documents.\n",
        "\n",
        "II. RELATED WORK\n",
        "\n",
        "Extractive domain-specific methods: A number of domain-specific methods have been created especially for the purpose of summarizing court documents. LetSum [1] and KMM [2], two unsupervised approaches that rank sentences based on term distribution models (TF-IDF and k-mixture model, respectively), were developed by Farzindar and Lapalme and Saravanan et al. CaseSummerizer [3], developed by Polsley et al., ranks sentences based on a combination of legal domain-specific features and TF-IDF weights. MMR [4], first presented by Zhong et al., uses a Maximum Margin Relevance module in conjunction with a 2-stage classifier to generate template-based summaries. In a comparative analysis of automated systems (LetSum, CaseSummerizer, Graphical Method (CRF)) for text summarization of legal case documents, Naimoonisa and Ankur [5] discovered that the Graphical Method performed the best.\n",
        "\n",
        "Abstractive methods: A lot of models for abstractive summarization have input token restrictions that are usually shorter than court case documents. Zhang et al. created Pegasus [ [8], See et al. suggested the Pointer-Generator model [6], Liu and Lapata presented BERTSumAbs [7], and Lewis et al. produced BART [9]. For these models, the maximum number of input tokens is often 1024. Longformer [10], which Beltagy et al. created to handle larger texts, can summarize long documents with up to 16 × 1024 input tokens. Using a pre-trained BART model over significant phrases, Bajaj et al.[ [11] presented a two-step extractive-abstractive strategy to summarize long documents through compression. A divide and conquer method was presented by Gidiotis and Tsoumakas [12] for sentenceby-sentence summarization of long materials. To the best of our knowledge, the sole method for abstractive legal document summarizing is LegalSumm [13].\n",
        "\n",
        "A supervised technique for abstractive summarization utilizing the T5 transformer was presented by Priyanka et al.\n",
        "\n",
        "[14]. In their comparison of the BART model's effectiveness with BERT, T5, and Roberta, Srividya et al. [15]discovered that BART is the most effective model for abstractive summarization. Anirban et al. [16] proposed a hybrid approach for abstractive summarization using BERT and GPT2.\n",
        "\n",
        "Research on abstractive text summarization in Indian legal documents is limited due to the use of generic datasets. Current models may not be optimized for the unique language, structure, and terminologies in multilingual and domain-specific documents. A research gap exists for a specialized abstractive summarization model using advanced models like XLNet and BART.\n",
        "\n",
        "III. PRELIMINARIES\n",
        "\n",
        "A. Text summarization\n",
        "\n",
        "Text summarization is the process of creating a concise, accurate, and eloquent synopsis of a longer text. Two different approaches to text summarization exist:\n",
        "\n",
        "· Extractive Text Summarization\n",
        "\n",
        "· Abstractive Text Summarization\n",
        "\n",
        "1) Extractive Text Summarization: The first method of text summarization to be developed was this one. This technique's primary goal is to extract the text's most significant sentences, which will then be included in the final summary. The same sentences from the original text are repeated in this summary.\n",
        "\n",
        "2) Abstractive Text Summarization: The extractive text summarization approach is expanded upon by this kind of text summarization. Additionally, this will extract the key phrases from the longer text, redefine it, and generate it in a fresh manner. It creates a summary that is as brief as possible.\n",
        "\n",
        "B. BERT\n",
        "\n",
        "BERT refers to transformer-based bidirectional encoder representations. We can easily complete the NLP tasks with this pre-trained model. To get around the shortcomings of LSTMs and RNNs, Bert is introduced. Masked language modeling and next sequence prediction are the two distinct training methodologies used by BERT. In Masked Language Modelling, a unique token known as a 'mask' is substituted for a randomly chosen set of tokens from the input text. BERT's goal is to forecast the masked tokens. Typically, 15% of the entire text is chosen for replacement by BERT. Within the 15% of the chosen text, 80% of the tokens are hidden, 10% stay unaltered, and the remaining 10% are substituted with a randomly chosen vocabulary that aligns with the original content. Determining if two sentences follow one another is the goal of Next Sentence Prediction. within the source text. In this instance, the input is sent in sentences, with the CLS tag coming before and the SEP tag after each phrase. The sentences serve as the basis for processing the full text.\n",
        "\n",
        "Two main tasks have been used to train the GPT-2 model: multiple-choice prediction and language modeling. In a language modeling assignment, the model can predict the next word based on context and prior words. When given keywords as inputs in a multiple-choice task, the model should be able to choose the right summary from among several summary sets. Every task has a certain amount of loss. The GPT2 model, which was created especially for text generation, predicts the nth token using an auto-regressive method that takes advantage of the context of the preceding n-1 tokens. The masked self-attention mechanism, which stops information created by tokens on the right side of present place from getting calculated, is a crucial component of the model.\n",
        "\n",
        "The GPT-2 model uses a particular token to specify the context in which the information that comes after it is to be summarized in order to produce a summary. The model is able to identify this context clue with some fine-tuning. By using this method, the model can determine at the end of the text which information is unnecessary and which is the intended summary. This training technique has become extremely popular recently for training language models that concentrate on interpreting language that is legible by humans.\n",
        "\n",
        "The goal of abstractive summarization is to create summaries based on a list of keywords, which sets it apart from extractive summarization. NLTK part-of-speech tagging and other similar methods are commonly used to identify keywords used in abstractive summarization. Tools like NLTK part-of-speech tagging are commonly used to identify keywords used in abstractive summarization. To train the GPT-2 model, the keywords are grouped into categories such as verbs, nouns, or a combination of both, and matched with human-generated abstracts, or gold summary abstracts.\n",
        "\n",
        "D. BART\n",
        "\n",
        "BERT uses Bidirectional Autoencoding technique whereas GPT-2 is Unidirectional Auto-regressive model, BART combines the important characteristics of both and forms Bidirectional Auto-regressive model. BART (Bidirectional and Auto-Regressive Transformers) is a state-of-the-art model for abstractive text summarization. It combines the power of both auto-regressive and auto-encoder architectures to generate coherent and contextually accurate summaries of input text. BART consists of a decoder and an encoder. In order to represent the text in a way that is accessible and intelligible by humans, the encoder extracts the relevant information from the provided text, and the decoder determines the likelihood of the following word. Multi-head attention is employed in the encoder of the BART system, whereas masked multi-head attention is used in the decoder. The purpose of utilizing a decoder is to ascertain the output sentence sequence by utilizing the conceptual information that has been retrieved from the encoder.\n",
        "\n",
        "· Masked multi head attention: The abstractive summary is created after extracting conceptual data from the encoder,\n",
        "\n",
        "which may contain noise and not be in chronological order. The decoder sequentially arranges the extracted information and predicts the likelihood of the next word using input text and output. The decoder is trained to predict the next word in the sequence. To address issues with the first token, the beginning of the statement (BOS) is added at the beginning. Attention is given to tokens up to the current position in the masked multi-head attention layer.\n",
        "\n",
        "E. XLNet\n",
        "\n",
        "BERT outperforms autoregressive language modeling-based pretraining techniques because it can mimic bidirectional circumstances. However, because BERT relies on masks to manipulate the input, it overlooks the link between the masked locations and suffers from a pretrain-finetune discrepancy.\n",
        "\n",
        "XLNet is a generalized autoregressive pretraining approach that allows learning in bidirectional situations by maximizing the expected probability over all factorization order permutations and, by virtue of its autoregressive formulation, solves the shortcomings of BERT.\n",
        "\n",
        "Moreover, Transformer-XL, the most advanced autoregressive model, is integrated into pretraining by XLNet. Empirically, on 20 tasks (question answering, natural language inference, emotion analysis, text summarization, and so forth) XLNet performs substantially better than BERT under similar trial settings.\n",
        "\n",
        "XLNet's ability to capture rich context and relationships between words in a document can make it effective for extractive summarization by assigning importance scores to sentences. However, it's important to note that extractive summarization with models like XLNet does not involve content generation but focuses on selecting and presenting existing content in a coherent summary.\n",
        "\n",
        "IV. PROPOSED METHODOLOGY\n",
        "\n",
        "Pairing XLNet with BART for abstractive text summarization can lead to improved results by leveraging XLNet's capabilities for understanding context and BART's strength in generating coherent and concise summaries. Here's an in-depth methodology for this approach:\n",
        "\n",
        "A. Data Preprocessing\n",
        "\n",
        "· Collect a large dataset of text documents that you want to summarize.\n",
        "\n",
        "· Preprocess the data by cleaning and tokenizing the text. Ensure that the data is in a format suitable for the input requirements of XLNet and BART.\n",
        "\n",
        "B. Fine-tuning XLNet for Extractive Summarization\n",
        "\n",
        "· Fine-tune the XLNet model on a dataset for extractive summarization. Train the model to identify and rank important sentences in the input document.\n",
        "\n",
        "· Use appropriate evaluation metrics such as ROUGE to assess the performance of the fine-tuned XLNet model.\n",
        "\n",
        "C. Generating Extractive Summaries\n",
        "\n",
        "· Utilize the fine-tuned XLNet model to generate extractive summaries from the input documents. These summaries will serve as an intermediate representation of the most salient information in the original text.\n",
        "\n",
        "D. Data Preprocessing for BART\n",
        "\n",
        "· Prepare the extractive summaries generated by XLNet as inputs for the BART model. Make sure the data is properly formatted for the BART model's input requirements.\n",
        "\n",
        "E. Fine-tuning BART for Abstractive Summarization\n",
        "\n",
        "· Fine-tune the BART model on a dataset that includes the paired original documents and their corresponding extractive summaries.\n",
        "\n",
        "· Train the BART model to generate more concise and coherent abstractive summaries based on the extractive summaries and the original text.\n",
        "\n",
        "F. Evaluation and Optimization\n",
        "\n",
        "· Evaluate the performance of the combined XLNet-BART model using appropriate metrics, such as ROUGE or BLEU scores, to measure the quality and similarity of the generated summaries to the reference summaries.\n",
        "\n",
        "· Optimize the model parameters, hyperparameters, and training strategies to achieve the best possible performance.\n",
        "\n",
        "G. Testing and Validation\n",
        "\n",
        "· Test the XLNet-BART model on a separate validation dataset to ensure that it generalizes well to new data and produces high-quality summaries consistently.\n",
        "\n",
        "V. EVALUATION BASED ON METRICS\n",
        "\n",
        "A. Rouge score\n",
        "\n",
        "ROUGE is a widely used evaluation statistic for machine translations, specifically text summarization. It compares the machine-generated summary and original text to determine the score. The score is calculated using the n-gram idea, with different versions based on unigrams, bigrams, trigrams, and longest common subsequence. ROUGE-1 uses unigrams, ROUGE-2 uses bigrams, ROUGE-3 uses trigrams, and\n",
        "\n",
        "ROUGE-L uses the longest common subsequence. Each score calculates recall, precision, and f1 score.\n",
        "\n",
        "B. BLEU Score\n",
        "\n",
        "The BLEU measure is used to compare human translation against machine translation. It compares the n-gram of reference translations or human translations to that of machine translations. BLEU is the first to have a greater correlation between reference and human translation, with an output value of 1 signifying a significant connection between human and machine-generated translations.\n",
        "\n",
        "VI. CONCLUSION\n",
        "\n",
        "The development of advanced transformer-based models, such as XLNet and BART, has significantly improved the field of abstractive text summarization. XLNet and BART have demonstrated superior capabilities in generating coherent and concise abstractive summaries, capturing complex language structures and preserving the original content's context and coherence. BART's denoising sequence-to-sequence pre-training approach has also produced accurate and meaningful abstractive summaries. Despite their proficiency in natural language processing tasks, these models often struggle to capture longrange dependencies and generate coherent and contextually accurate summaries. XLNet and BART are more adept at understanding the nuances of input text, resulting in more precise and contextually rich summaries. Given the dynamic nature of text summarization and the constant evolution of transformer-based models, it is crucial for researchers and practitioners to continue exploring the potential of XLNet and BART to further enhance abstractive text summarization capabilities and meet the growing demands for precise and contextually relevant summarization in various domains.\n",
        "\n",
        "REFERENCES\n",
        "\n",
        "[1] Farzindar, A.., Lapalme, G. (2004). LetSum: Automatic text summarization of on-line news. In Proceedings of the ACL Interactive Poster and Demonstration Sessions (p. 31).\n",
        "\n",
        "[2] Saravanan, M., Umamaheswari, K. (2006). Text summarization using k-mixture model. In Proceedings of the Workshop on Multi-source Multilingual Information Extraction and Summarization (p. 38).\n",
        "\n",
        "[3] Polsley, S., Zak, S., Dredze, M. (2016). CaseSummerizer: A tool for case law summarization. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) (pp. 7-12).\n",
        "\n",
        "[4] Zhong, L., Xia, R., Li, J. (2019). MMR: A maximum margin relevance model for extractive summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 20072017).\n",
        "\n",
        "[5] Naimoonisa and Ankur. (2021). Classification of automated systems for text summarization of legal case documents. In Proceedings of the IEEE International Conference on Advances in Computing, Communication and Automation (ICACCA) (pp. 301-306).\n",
        "\n",
        "[6] See, A., Liu, P. J., Manning, C. D. (2017). Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 1073-1083).\n",
        "\n",
        "[7] Liu, Y., Lapata, M. (2019). Text summarization with pretrained encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (pp. 3721-3731).\n",
        "\n",
        "[8] Zhang, J., Lapata, M. (2020). PEGASUS: Pre-training with extracted gap-sentences for abstractive text summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (pp. 1101-1113).\n",
        "\n",
        "[9] Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., ... Zettlemoyer, L. (2019). BART: Denoising sequence-tosequence pre-training for natural language understanding. arXiv preprint arXiv:1910.13461.\n",
        "\n",
        "[10] Beltagy, I., Peters, M. E., Cohan, A. (2020). Longformer: The longdocument transformer. arXiv preprint arXiv:2004.05150.\n",
        "\n",
        "[11] Bajaj, P., Aggarwal, M., Chhabra, A. (2021). Summarizing long legal documents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 2757-2768).\n",
        "\n",
        "[12] Gidiotis, G., Tsoumakas, G. (2020). Divide and conquer for extractive summarization of lengthy documents. In Proceedings of the 42nd European Conference on Information Retrieval (ECIR) (pp. 178-192).\n",
        "\n",
        "[13] Feijo, R., Moreira, V. (2021). LegalSumm: A dataset for abstractive legal document summarization. arXiv preprint arXiv:2106.04964.\n",
        "\n",
        "[14] Priyanka, T., Pankaj, R., Singh, P. (2022). Abstractive summarization of legal documents using the T5 transformer. In Proceedings of the IEEE International Conference on Data, Information and Knowledge Management (CIKM) (pp. 295-300).\n",
        "\n",
        "[15] Srividya, S., Ramachandran, A., Narayanan, A. (2022). Comparative analysis of transformer-based models for abstractive legal document summarization. In Proceedings of the IEEE International Conference on Data, Information and Knowledge Management (CIKM) (pp. 301-306).\n",
        "\n",
        "[16] Anirban et al. (2023). A hybrid approach for abstractive summarization of legal documents using BERT and GPT2. In Proceedings of the IEEE International Conference on Natural Language Processing and Knowledge Engineering (NLPKE) (pp. 1-5).\n",
        "\"\"\"\n",
        "print(\"Generated Summary:\\n\", summarize(text, model, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldGyh1XkNrrZ",
        "outputId": "19ba406a-b7ef-4aab-b78e-461a887422d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            " This study advances our understanding of abstractive summarization, particularly in the realm of legal texts .\n",
            "The traditional, labor-intensive process of manual case summarization can be revolutionized with the aid of state-of-the-art machine learning models .\n",
            "With over 4.70 crore pending legal cases in India, automatic summarization holds the potential to streamline and expedite legal proceedings significantly .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_JCeONlWz-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install evaluate rouge\n",
        "!pip install rouge_score\n",
        "!pip install  rouge\n",
        "!pip install evaluate\n",
        "!pip install fsspec==2023.6.0"
      ],
      "metadata": {
        "id": "3YQXFjmuNryJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39975c08-599d-4bcd-d58d-d8890c73aa29"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge) (1.17.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.32.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Collecting fsspec==2023.6.0\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.6.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed fsspec-2023.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "fsspec"
                ]
              },
              "id": "922cf3c7224c4d0f9a8271a455e67231"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6bh7GDaTphY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
